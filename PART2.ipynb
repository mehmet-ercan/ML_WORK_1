{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93d4afc0cc5d21fd",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Import Librarires And Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c3980ae6c078aa64",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T07:39:32.310721Z",
     "start_time": "2024-04-02T07:39:32.304604Z"
    }
   },
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "#some settings to show data\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', 50)\n",
    "\n",
    "#import dataset\n",
    "audit_risk = pd.read_csv(\"datasets/audit_risk.csv\")\n",
    "trial = pd.read_csv(\"datasets/trial.csv\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "a4bb796ede4ad52a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Show Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "91d85799c25bbbf9",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T07:39:33.052460Z",
     "start_time": "2024-04-02T07:39:33.036732Z"
    }
   },
   "source": [
    "audit_risk.head(10)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a9182022dbeca44f",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T07:39:33.079356Z",
     "start_time": "2024-04-02T07:39:33.071048Z"
    }
   },
   "source": [
    "trial.head(10)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "a66b5d44b7351844",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Lets See Values Of Two Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a73aeaacff2883a6",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T07:39:33.141017Z",
     "start_time": "2024-04-02T07:39:33.114358Z"
    }
   },
   "source": [
    "audit_risk.describe()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "40615ed3b458df04",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T07:39:33.179070Z",
     "start_time": "2024-04-02T07:39:33.162032Z"
    }
   },
   "source": [
    "trial.describe()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "647fe5f5ef79d0f0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Analysis**\n",
    "\n",
    "As you can see, two dataset are similarly same expect a bit difference. \n",
    "Firsty, SCORE_A AND SCORE_B in trial, multiply 10 with audit_risk Score_A and Score_B values, also that's capital. \n",
    "Second, Loss and Risk column in trial, completely different from audit_risk.\n",
    "\n",
    "First of all, change capital column names like audit_risk columns, then divide by 10 to Score_A and Score_B;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ea1c187784d5b33d",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T07:39:33.268948Z",
     "start_time": "2024-04-02T07:39:33.266876Z"
    }
   },
   "source": [
    "trial.columns = ['Sector_score', 'LOCATION_ID', 'PARA_A', 'Score_A', 'PARA_B',\n",
    "                 'Score_B', 'TOTAL', 'numbers', 'Marks',\n",
    "                 'Money_Value', 'MONEY_Marks', 'District',\n",
    "                 'Loss', 'LOSS_SCORE', 'History', 'History_score', 'Score', 'Risk_trial']"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "98a65e9495d4e3ca",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T07:39:33.332500Z",
     "start_time": "2024-04-02T07:39:33.330010Z"
    }
   },
   "source": [
    "trial['Score_A'] = trial['Score_A'] / 10\n",
    "trial['Score_B'] = trial['Score_B'] / 10"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "75b36c0b2d8f929b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Observe two dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "cc9a73a12324aa74",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T07:39:33.423719Z",
     "start_time": "2024-04-02T07:39:33.421095Z"
    }
   },
   "source": [
    "same_columns = np.intersect1d(audit_risk.columns, trial.columns)\n",
    "same_columns"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "5fb3034f267dc364",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Let's merge two dataset with same column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "4031d1cf103bf025",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T07:39:33.499263Z",
     "start_time": "2024-04-02T07:39:33.493955Z"
    }
   },
   "source": [
    "merged_df = pd.merge(audit_risk, trial, how='outer',\n",
    "                     on=['History', 'LOCATION_ID', 'Money_Value', 'PARA_A', 'PARA_B', 'Score', 'Score_A', 'Score_B',\n",
    "                         'Sector_score', 'TOTAL', 'numbers'])\n",
    "merged_df.columns"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "55df7ce7fb8adccf",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Analysis**\n",
    "\n",
    "As you can see some values in Risk_trial in trial and Risk in audit_risk are different, we can select Risk column in audit_risk because if you will click link https://api.openml.org/d/42931, you can see target value is Risk in audit_risk dataset. So delete that column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a49b37371871b6a6",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T07:39:33.620695Z",
     "start_time": "2024-04-02T07:39:33.618314Z"
    }
   },
   "source": [
    "df = merged_df.drop(['Risk_trial'], axis=1)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "bd5bcc8444f2955a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Check null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "65be6230dcfbb06f",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T07:39:33.659544Z",
     "start_time": "2024-04-02T07:39:33.656201Z"
    }
   },
   "source": [
    "df.isnull().sum()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "90fd5295d73d4eaf",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "As you can see, Money_Value column has a null value. Set average value,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ea0238ae1b1e1ba7",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T07:39:33.739113Z",
     "start_time": "2024-04-02T07:39:33.736775Z"
    }
   },
   "source": [
    "df['Money_Value'] = df['Money_Value'].fillna(df['Money_Value'].median())"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "2c5e43d2ebaad018",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "and Detection_Risk column is same value of Risk column, so delete it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "d667112a038772cd",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T07:39:33.758214Z",
     "start_time": "2024-04-02T07:39:33.752826Z"
    }
   },
   "source": [
    "df = df.drop(['Detection_Risk'], axis=1)\n",
    "df.info()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "e711c4773c62d91b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Up to now, everything is good, let's see location id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "afb18b60505b06c3",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T07:39:33.780607Z",
     "start_time": "2024-04-02T07:39:33.778037Z"
    }
   },
   "source": [
    "df[\"LOCATION_ID\"].unique()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "d3cd2dc5773a86b0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "if you iterate to showed values, you will see end of the table there are some non numeric values, LOHARU, NUH and SAFIDON. How much that values in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "1440609eb3c3407e",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T07:39:33.840407Z",
     "start_time": "2024-04-02T07:39:33.837244Z"
    }
   },
   "source": [
    "len(df[(df[\"LOCATION_ID\"] == 'LOHARU') | (df[\"LOCATION_ID\"] == 'NUH') | (df[\"LOCATION_ID\"] == 'SAFIDON')])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "86b1dcf5db5c4829",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T07:39:33.872601Z",
     "start_time": "2024-04-02T07:39:33.870232Z"
    }
   },
   "source": [
    "len(df)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "5204ca86ad943dbc",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Only 3 rows we have non numerical rows, so they seem deletable, i deleted it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "92e7e1fd3329f41b",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T07:39:33.954472Z",
     "start_time": "2024-04-02T07:39:33.950877Z"
    }
   },
   "source": [
    "df = df[(df.LOCATION_ID != 'LOHARU')]\n",
    "df = df[(df.LOCATION_ID != 'NUH')]\n",
    "df = df[(df.LOCATION_ID != 'SAFIDON')]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "12710a869bbc1361",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T07:39:33.992753Z",
     "start_time": "2024-04-02T07:39:33.990472Z"
    }
   },
   "source": [
    "len(df)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "956ef4fd06508190",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Also i drop duplicate values,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "eb3eb29e96bf7ce5",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T07:39:34.070471Z",
     "start_time": "2024-04-02T07:39:34.065860Z"
    }
   },
   "source": [
    "df = df.drop_duplicates(keep='first')\n",
    "print(f\"Rows: {len(df)}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "3d1bdc6a040a4785",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "i drop high correlation values;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "f20d7b9e3d5467fa",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T07:39:34.123961Z",
     "start_time": "2024-04-02T07:39:34.096335Z"
    }
   },
   "source": [
    "import seaborn as sns\n",
    "\n",
    "corr = df.corr()\n",
    "corr.style.background_gradient(cmap='coolwarm')\n",
    "# 'RdBu_r' & 'BrBG' are other good diverging colormaps\n",
    "cm = sns.diverging_palette(220, 20, sep=20, as_cmap=True)\n",
    "corr.style.background_gradient(cmap=cm)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "23e45375f6c23ef",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T07:39:34.159889Z",
     "start_time": "2024-04-02T07:39:34.146926Z"
    }
   },
   "source": [
    "df = df[['Risk_A', 'Risk_B', 'Risk_C', 'Risk_D', 'RiSk_E', 'Prob', 'Score', 'CONTROL_RISK', 'Audit_Risk', 'Risk', 'MONEY_Marks', 'Loss']]\n",
    "corr = df.corr()\n",
    "corr.style.background_gradient(cmap='coolwarm')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "64e56cee814993d3",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T07:39:34.199924Z",
     "start_time": "2024-04-02T07:39:34.190730Z"
    }
   },
   "source": [
    "df"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "2665b2c6640b9edf",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Data Clean Operation Is Done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93698df684ef540e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# I will Implement Knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "f4f703ccf2472fd0",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T07:39:34.736541Z",
     "start_time": "2024-04-02T07:39:34.734356Z"
    }
   },
   "source": [
    "import math\n",
    "\n",
    "# Define a function to calculate the Euclidean distance between two points\n",
    "def euclidean_distance(x1, x2):\n",
    "    return math.sqrt(np.sum((x1 - x2) ** 2))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "d154e3bb7d3ebb6e",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T07:39:34.765921Z",
     "start_time": "2024-04-02T07:39:34.762997Z"
    }
   },
   "source": [
    "# Define the KNN function\n",
    "def knn_classification_with_euclidean_distance(X_train, y_train, X_test, k):\n",
    "    # List to store the predicted labels for the test set\n",
    "    y_pred = []\n",
    "    distances = []\n",
    "    \n",
    "    for i in range(len(X_test)):\n",
    "        for j in range(len(X_train)):\n",
    "            # Calculate the distance between the two points using euclidean_distance func where I defined above section\n",
    "            dist = euclidean_distance(X_test[i], X_train[j])\n",
    "            distances.append((dist, y_train[j]))\n",
    "\n",
    "        distances.sort()\n",
    "        neighbors = distances[:k] # Get the k nearest neighbors\n",
    "\n",
    "        counts = {} # Count the votes for each class\n",
    "        for neighbor in neighbors:\n",
    "            label = neighbor[1]\n",
    "            if label in counts:\n",
    "                counts[label] += 1\n",
    "            else:\n",
    "                counts[label] = 1\n",
    "\n",
    "        max_count = max(counts, key=counts.get) # Get the class with the most votes\n",
    "        y_pred.append(max_count)\n",
    "\n",
    "    return y_pred"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "2dd97d72e3d65edf",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T07:39:34.812673Z",
     "start_time": "2024-04-02T07:39:34.810709Z"
    }
   },
   "source": [
    "# Define a function to calculate the Manhattan distance between two points\n",
    "def manhattan_distance(x1, x2):\n",
    "    return np.sum(np.abs(x1 - x2))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "eed31be7d7f63a06",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T07:39:34.858081Z",
     "start_time": "2024-04-02T07:39:34.855566Z"
    }
   },
   "source": [
    "def knn_regressor_with_manhattan_distance(X_train, y_train, X_test, k):\n",
    "    y_pred = []\n",
    "    distances = []\n",
    "    \n",
    "    for i in range(len(X_test)):\n",
    "        for j in range(len(X_train)):\n",
    "            # Calculate the distance between the two points using manhattan_distance func where I defined above section\n",
    "            dist = manhattan_distance(X_test[i], X_train[j])\n",
    "            distances.append((dist, y_train[j]))\n",
    "\n",
    "        distances.sort()\n",
    "        neighbors = distances[:k]# Get the k nearest neighbors\n",
    "        \n",
    "        mean_val = np.mean(neighbors)# Get the mean from the neighbors\n",
    "        y_pred.append(mean_val)\n",
    "\n",
    "    return y_pred"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "**I finished preprocessing to data. I will go to implementing functions, start Part1**",
   "metadata": {
    "collapsed": false
   },
   "id": "dcd13bdafe5ecbc7"
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "34c465f71f36b4ba",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T07:39:34.930725Z",
     "start_time": "2024-04-02T07:39:34.927733Z"
    }
   },
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class_df = df.drop(\"Audit_Risk\", axis=1)\n",
    "classification_X = class_df.drop([\"Risk\"], axis=1)\n",
    "classification_y = class_df[\"Risk\"]"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**I am seperate my data for train %70 and test %30, so i will use train_test_split func in model_selection library**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9f950fd060f677f2"
  },
  {
   "cell_type": "code",
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(classification_X, classification_y, test_size=0.3, random_state=42)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T07:39:34.958076Z",
     "start_time": "2024-04-02T07:39:34.955303Z"
    }
   },
   "id": "f22c84380429902e",
   "execution_count": 108,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "83864c495304bfbf",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# PART 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "82dc85dd7291e0e5",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T07:39:38.381681Z",
     "start_time": "2024-04-02T07:39:38.356212Z"
    }
   },
   "source": [
    "bike = pd.DataFrame(pd.read_csv(\"datasets/day.csv\"))\n",
    "print(bike.head())\n",
    "print(bike.info())\n",
    "print(bike.describe())\n",
    "print(bike.shape)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "3854053951ee8fbd",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Conclusion of Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Dataset has 730 rows and 16 columns.\n",
    "Except one column, all others are either float or integer type.\n",
    "One column is date type.\n",
    "\n",
    "Looking at the data, it seems to be some fields that are categorical, but in integer/float type.\n",
    "We will analyse to convert them to categorical as integer."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b57c68452434910c"
  },
  {
   "cell_type": "code",
   "source": [
    "round(100 * (bike.isnull().sum() / len(bike)), 2).sort_values(ascending=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T07:39:38.385367Z",
     "start_time": "2024-04-02T07:39:38.382240Z"
    }
   },
   "id": "72b201c81f94d20a",
   "execution_count": 115,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "668a63736d999c4b",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T07:39:38.389109Z",
     "start_time": "2024-04-02T07:39:38.386031Z"
    }
   },
   "source": [
    "round((bike.isnull().sum(axis=1) / len(bike)) * 100, 2).sort_values(ascending=False)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "8f1b335fbbd0bd76",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Analysis**\n",
    "\n",
    "There are no missing / Null values either in columns or rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "dd56c3c584b2c4e8",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T07:39:38.392714Z",
     "start_time": "2024-04-02T07:39:38.389608Z"
    }
   },
   "source": [
    "bike_dup = bike.copy()\n",
    "bike_dup = bike_dup.drop_duplicates(keep='first')\n",
    "# we can assume same operation like this => bike_dup.drop_duplicates(subset=None, inplace=True)\n",
    "\n",
    "print(bike_dup.shape)\n",
    "print(bike.shape)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "5ea91920bb53d5aa",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Analysis**\n",
    "The shape after running the drop duplicate command is same as the original dataframe.\n",
    "Hence we can conclude that there were zero duplicate values in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "74610ab2aa09e46a",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T07:39:38.401570Z",
     "start_time": "2024-04-02T07:39:38.393872Z"
    }
   },
   "source": [
    "bike_dummy = bike.iloc[:, 1:16]\n",
    "\n",
    "for col in bike_dummy:\n",
    "    print(bike_dummy[col].value_counts(ascending=False), '\\n\\n\\n')"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "fef2dbe3e7b76b1",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**As you can see on above code output;**\n",
    "\n",
    "instant, dteday, casual and registered columns are nonessential, so we can remove these columns. Because;\n",
    "\n",
    "*instant* : its index,\n",
    "*dteday* : it has the date,\n",
    "*casual & registered* : i dont consider the columns that specify bike counts by customer categories since our objective is to determine the total bike count. Furthermore, we've introduced a new variable to represent the proportion of different customer types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "53dc785cfcb86890",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T07:39:38.404232Z",
     "start_time": "2024-04-02T07:39:38.402204Z"
    }
   },
   "source": [
    "bike_new = bike[\n",
    "    ['season', 'yr', 'mnth', 'holiday', 'weekday', 'workingday', 'weathersit', 'temp', 'atemp', 'hum', 'windspeed', 'cnt']]\n",
    "# bike_new = bike[['temp', 'atemp', 'hum', 'windspeed', 'cnt']] we can see that columns are numerical"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "c2d720ec66e9c8c2",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T07:39:38.411934Z",
     "start_time": "2024-04-02T07:39:38.404823Z"
    }
   },
   "source": [
    "bike_new.info()\n",
    "bike_new.head()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "f0775d8b24a3c761",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Creating Dummy Variables**\n",
    "I can drop all categorical data, so ['temp', 'atemp', 'hum', 'windspeed', 'cnt'] columns are usefull. But i can create dummy variable. \n",
    "Dummy variables are usefull because they allow us to include categorical variables in our analysis, which would otherwise be difficult to include due to their non-numeric nature. \n",
    "They can also help us to control for confounding factors and improve the validity of our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "8deaebca0ab06029",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T07:39:38.415945Z",
     "start_time": "2024-04-02T07:39:38.412630Z"
    }
   },
   "source": [
    "bike_new['season'] = bike_new['season'].astype('category')\n",
    "bike_new['weathersit'] = bike_new['weathersit'].astype('category')\n",
    "bike_new['mnth'] = bike_new['mnth'].astype('category')\n",
    "bike_new['weekday'] = bike_new['weekday'].astype('category')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "f9dee1661950f0c8",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T07:39:38.422064Z",
     "start_time": "2024-04-02T07:39:38.416518Z"
    }
   },
   "source": [
    "bike_new = pd.get_dummies(bike_new, drop_first=True)\n",
    "bike_new.info()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "da16c5eaa6ad11b0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Go\n",
    "\n",
    "Splitting the data to Train and Test: - I am splitting the data into TRAIN and TEST (70:30 ratio), now,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "95c6c4ffbe39a298",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T07:39:38.425379Z",
     "start_time": "2024-04-02T07:39:38.422600Z"
    }
   },
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "regression_df = bike_new\n",
    "regression_X = regression_df.drop([\"cnt\"], axis=1)\n",
    "regression_y = regression_df[\"cnt\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(regression_X, regression_y, test_size=0.3, random_state=42)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "2ab08ba2cb1a54ab",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Perform K-NN for k = 3 With K-fold Cross Validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "8724da9995a6b18b",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T07:39:39.627376Z",
     "start_time": "2024-04-02T07:39:38.425865Z"
    }
   },
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert y_train and y_test to numpy arrays for using in knn_classification_with_euclidean_distance func\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "kf = KFold(n_splits=6, shuffle=True, random_state=42)\n",
    "\n",
    "r2_values = []\n",
    "mse_values = []\n",
    "rmse_values = []\n",
    "\n",
    "# Perform k-fold cross-validation\n",
    "for train_index, val_index in kf.split(X_train_scaled):\n",
    "    X_train_fold, X_val_fold = X_train_scaled[train_index], X_train_scaled[val_index]\n",
    "    y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n",
    "\n",
    "\n",
    "    start_time = time.time()\n",
    "    # Predict using KNN regression\n",
    "    y_pred_fold = knn_regressor_with_manhattan_distance(X_train_fold, y_train_fold, X_val_fold, 3)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    # mean squared error, r2 score\n",
    "    # r2 = r2_score(y_val_fold, y_pred_fold)\n",
    "    mse = mean_squared_error(y_val_fold, y_pred_fold)\n",
    "    rmse = np.sqrt(mse)\n",
    "\n",
    "    # r2_values.append(r2)\n",
    "    mse_values.append(mse)\n",
    "    rmse_values.append(rmse)\n",
    "    \n",
    "# Calculate average mean\n",
    "# print(\"Average R2 Score:\", np.mean(r2_values))\n",
    "print(f\"Average Mean Squared Error: {np.mean(mse_values)}\")\n",
    "print(f\"Average Root Mean Squared error: {np.mean(rmse_values)}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X_train_scaled, y_train)\n",
    "k_neighbors_predictions = knn.predict(X_test_scaled)\n",
    "accuracy_score(y_test, k_neighbors_predictions)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T07:39:39.657556Z",
     "start_time": "2024-04-02T07:39:39.628373Z"
    }
   },
   "id": "f046fb38a5b18d5e",
   "execution_count": 125,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "d2c47bf8d426108f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Runtime Performance**"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Mean Squared Error (MSE) is a metric commonly used to evaluate the performance of a regression model. It measures the average of the squares of the errors, \n",
    "which are the differences between actual values and predicted values. \n",
    "\n",
    "MSE quantifies the average squared difference between actual values and predicted values. A smaller MSE indicates better agreement between the predicted and actual values, \n",
    "whereas a larger MSE suggests poorer model performance."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c4fc6fb755c137f5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
